{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e34e4aed-d079-4899-be2a-199b700e7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8113cb53-36a7-4e8f-a5d5-2250bec280a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the table if it doesn't exist\n",
    "def create_table():\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(\"vehicle_data.db\")\n",
    "    # Create a cursor object to execute SQL statements\n",
    "    c = conn.cursor()\n",
    "    # Create the \"vehicles\" table with two columns: \"vehicle_number\" and \"bike_image_path\"\n",
    "    # The \"IF NOT EXISTS\" clause ensures that the table is only created if it doesn't already exist\n",
    "    c.execute(\"CREATE TABLE IF NOT EXISTS vehicles (vehicle_number TEXT, bike_image_path TEXT)\")\n",
    "    # Commit the changes to the database\n",
    "    conn.commit()\n",
    "    # Close the database connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a286559-392c-4273-96cc-5408f93e87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert a record into the \"vehicles\" table\n",
    "def insert_record(vehicle_number, bike_image_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(\"vehicle_data.db\")\n",
    "    # Create a cursor object to execute SQL statements\n",
    "    c = conn.cursor()\n",
    "    # Insert the record into the \"vehicles\" table using parameterized SQL statement\n",
    "    c.execute(\"INSERT INTO vehicles VALUES (?, ?)\", (vehicle_number, bike_image_path))\n",
    "    # Commit the changes to the database\n",
    "    conn.commit()\n",
    "    # Close the database connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4344c7e-9163-487e-bc0a-c228b889dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLO models\n",
    "person_bike_model = YOLO(r\"Bike detectionbest.pt\")\n",
    "helmet_model = YOLO(r\"Helmet Detection.pt\")\n",
    "number_plate_model = YOLO(r\"Liscence Plate Detection.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9539da62-d0aa-4850-967d-2590914f8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"E:/python/Lib/site-packages/pytesseract\"  # Update with the path to your Tesseract OCR executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b30c4b89-fe5c-4aae-a5b8-729c88998c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"out\"  # Directory to save the output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1defce03-d5d5-4352-940d-3e8fea73bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up video capture\n",
    "video_capture = cv2.VideoCapture(0)  # Use 0 for the default camera or provide the desired camera index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa732793-0153-4233-8118-18bf0f69416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 128x224 1 Person_Bike, 512.0ms\n",
      "Speed: 5.0ms preprocess, 512.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 544.2ms\n",
      "Speed: 1.0ms preprocess, 544.2ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 460.9ms\n",
      "Speed: 0.1ms preprocess, 460.9ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 320.0ms\n",
      "Speed: 1.1ms preprocess, 320.0ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 524.1ms\n",
      "Speed: 2.0ms preprocess, 524.1ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 383.5ms\n",
      "Speed: 2.0ms preprocess, 383.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 426.0ms\n",
      "Speed: 1.0ms preprocess, 426.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 92.4ms\n",
      "Speed: 1.1ms preprocess, 92.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 128.0ms\n",
      "Speed: 1.0ms preprocess, 128.0ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 (no detections), 57.8ms\n",
      "Speed: 1.0ms preprocess, 57.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 135.3ms\n",
      "Speed: 0.0ms preprocess, 135.3ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 (no detections), 59.6ms\n",
      "Speed: 1.0ms preprocess, 59.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 112.6ms\n",
      "Speed: 1.0ms preprocess, 112.6ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 58.0ms\n",
      "Speed: 1.0ms preprocess, 58.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 65.8ms\n",
      "Speed: 1.0ms preprocess, 65.8ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 58.1ms\n",
      "Speed: 2.0ms preprocess, 58.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 78.5ms\n",
      "Speed: 1.0ms preprocess, 78.5ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 59.2ms\n",
      "Speed: 0.7ms preprocess, 59.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 69.2ms\n",
      "Speed: 1.0ms preprocess, 69.2ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 56.9ms\n",
      "Speed: 1.0ms preprocess, 56.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 81.3ms\n",
      "Speed: 0.5ms preprocess, 81.3ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 59.1ms\n",
      "Speed: 1.0ms preprocess, 59.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 137.2ms\n",
      "Speed: 0.9ms preprocess, 137.2ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 53.5ms\n",
      "Speed: 1.0ms preprocess, 53.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 110.8ms\n",
      "Speed: 1.0ms preprocess, 110.8ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 1 With Helmet, 56.0ms\n",
      "Speed: 0.9ms preprocess, 56.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 126.5ms\n",
      "Speed: 0.6ms preprocess, 126.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 55.0ms\n",
      "Speed: 1.0ms preprocess, 55.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 (no detections), 109.4ms\n",
      "Speed: 1.0ms preprocess, 109.4ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 (no detections), 65.3ms\n",
      "Speed: 1.0ms preprocess, 65.3ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 (no detections), 83.0ms\n",
      "Speed: 1.0ms preprocess, 83.0ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 59.6ms\n",
      "Speed: 1.1ms preprocess, 59.6ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 (no detections), 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 (no detections), 164.4ms\n",
      "Speed: 1.0ms preprocess, 164.4ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 (no detections), 45.0ms\n",
      "Speed: 1.0ms preprocess, 45.0ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 (no detections), 141.7ms\n",
      "Speed: 1.9ms preprocess, 141.7ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 76.2ms\n",
      "Speed: 1.0ms preprocess, 76.2ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 57.9ms\n",
      "Speed: 1.0ms preprocess, 57.9ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 65.5ms\n",
      "Speed: 0.9ms preprocess, 65.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 60.1ms\n",
      "Speed: 1.0ms preprocess, 60.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 80.1ms\n",
      "Speed: 0.0ms preprocess, 80.1ms inference, 0.7ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 1 With Helmet, 55.2ms\n",
      "Speed: 0.9ms preprocess, 55.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 158.3ms\n",
      "Speed: 1.0ms preprocess, 158.3ms inference, 1.2ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 1 With Helmet, 52.0ms\n",
      "Speed: 1.0ms preprocess, 52.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 85.5ms\n",
      "Speed: 1.0ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x160 (no detections), 85.5ms\n",
      "Speed: 1.0ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 160)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 69.5ms\n",
      "Speed: 0.0ms preprocess, 69.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x160 1 With Helmet, 71.0ms\n",
      "Speed: 1.0ms preprocess, 71.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 160)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 66.2ms\n",
      "Speed: 0.9ms preprocess, 66.2ms inference, 0.7ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 58.0ms\n",
      "Speed: 1.1ms preprocess, 58.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 88.6ms\n",
      "Speed: 0.9ms preprocess, 88.6ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 58.6ms\n",
      "Speed: 0.0ms preprocess, 58.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 159.0ms\n",
      "Speed: 1.0ms preprocess, 159.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 (no detections), 59.0ms\n",
      "Speed: 0.0ms preprocess, 59.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 (no detections), 189.2ms\n",
      "Speed: 1.0ms preprocess, 189.2ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 82.1ms\n",
      "Speed: 0.9ms preprocess, 82.1ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 (no detections), 55.8ms\n",
      "Speed: 0.0ms preprocess, 55.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 140.6ms\n",
      "Speed: 0.9ms preprocess, 140.6ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 (no detections), 55.5ms\n",
      "Speed: 0.0ms preprocess, 55.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 114.4ms\n",
      "Speed: 0.9ms preprocess, 114.4ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 60.0ms\n",
      "Speed: 1.0ms preprocess, 60.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 65.0ms\n",
      "Speed: 0.0ms preprocess, 65.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 61.0ms\n",
      "Speed: 1.0ms preprocess, 61.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 97.4ms\n",
      "Speed: 1.0ms preprocess, 97.4ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 58.1ms\n",
      "Speed: 1.0ms preprocess, 58.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 80.5ms\n",
      "Speed: 0.0ms preprocess, 80.5ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 1 With Helmet, 58.5ms\n",
      "Speed: 1.0ms preprocess, 58.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 62.7ms\n",
      "Speed: 0.4ms preprocess, 62.7ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 1 With Helmet, 56.3ms\n",
      "Speed: 1.0ms preprocess, 56.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 71.7ms\n",
      "Speed: 0.0ms preprocess, 71.7ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 (no detections), 58.9ms\n",
      "Speed: 1.0ms preprocess, 58.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 45.1ms\n",
      "Speed: 1.0ms preprocess, 45.1ms inference, 1.5ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x192 2 With Helmets, 56.8ms\n",
      "Speed: 0.0ms preprocess, 56.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 192)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 64.3ms\n",
      "Speed: 1.0ms preprocess, 64.3ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 57.2ms\n",
      "Speed: 0.0ms preprocess, 57.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 48.5ms\n",
      "Speed: 0.0ms preprocess, 48.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 68.5ms\n",
      "Speed: 1.0ms preprocess, 68.5ms inference, 0.9ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 58.9ms\n",
      "Speed: 1.0ms preprocess, 58.9ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 128.5ms\n",
      "Speed: 1.0ms preprocess, 128.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 57.5ms\n",
      "Speed: 0.0ms preprocess, 57.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 75.8ms\n",
      "Speed: 0.0ms preprocess, 75.8ms inference, 0.7ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 56.4ms\n",
      "Speed: 1.0ms preprocess, 56.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 88.8ms\n",
      "Speed: 1.0ms preprocess, 88.8ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 55.3ms\n",
      "Speed: 0.0ms preprocess, 55.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 62.7ms\n",
      "Speed: 1.0ms preprocess, 62.7ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 57.5ms\n",
      "Speed: 0.0ms preprocess, 57.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 52.7ms\n",
      "Speed: 1.0ms preprocess, 52.7ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 57.0ms\n",
      "Speed: 0.0ms preprocess, 57.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 47.8ms\n",
      "Speed: 0.0ms preprocess, 47.8ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 56.1ms\n",
      "Speed: 1.0ms preprocess, 56.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 60.0ms\n",
      "Speed: 0.0ms preprocess, 60.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 58.8ms\n",
      "Speed: 1.0ms preprocess, 58.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 50.0ms\n",
      "Speed: 1.0ms preprocess, 50.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 59.2ms\n",
      "Speed: 1.0ms preprocess, 59.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 66.5ms\n",
      "Speed: 0.0ms preprocess, 66.5ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 59.0ms\n",
      "Speed: 0.9ms preprocess, 59.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 45.0ms\n",
      "Speed: 1.0ms preprocess, 45.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 56.3ms\n",
      "Speed: 0.0ms preprocess, 56.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 126.0ms\n",
      "Speed: 1.0ms preprocess, 126.0ms inference, 0.8ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 54.5ms\n",
      "Speed: 0.0ms preprocess, 54.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 69.2ms\n",
      "Speed: 0.0ms preprocess, 69.2ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 56.1ms\n",
      "Speed: 1.0ms preprocess, 56.1ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 54.0ms\n",
      "Speed: 1.0ms preprocess, 54.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 58.1ms\n",
      "Speed: 0.0ms preprocess, 58.1ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 78.6ms\n",
      "Speed: 1.0ms preprocess, 78.6ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 58.1ms\n",
      "Speed: 0.9ms preprocess, 58.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 53.0ms\n",
      "Speed: 1.0ms preprocess, 53.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 63.1ms\n",
      "Speed: 1.0ms preprocess, 63.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 55.1ms\n",
      "Speed: 0.0ms preprocess, 55.1ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 60.1ms\n",
      "Speed: 1.0ms preprocess, 60.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 53.1ms\n",
      "Speed: 1.0ms preprocess, 53.1ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 57.0ms\n",
      "Speed: 0.0ms preprocess, 57.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 58.6ms\n",
      "Speed: 1.0ms preprocess, 58.6ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 57.5ms\n",
      "Speed: 0.0ms preprocess, 57.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 94.8ms\n",
      "Speed: 0.0ms preprocess, 94.8ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 62.3ms\n",
      "Speed: 0.0ms preprocess, 62.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 51.0ms\n",
      "Speed: 0.0ms preprocess, 51.0ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 56.6ms\n",
      "Speed: 0.0ms preprocess, 56.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 61.7ms\n",
      "Speed: 1.0ms preprocess, 61.7ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 60.5ms\n",
      "Speed: 1.0ms preprocess, 60.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 53.7ms\n",
      "Speed: 0.0ms preprocess, 53.7ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 69.4ms\n",
      "Speed: 1.0ms preprocess, 69.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 56.7ms\n",
      "Speed: 1.0ms preprocess, 56.7ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 61.7ms\n",
      "Speed: 1.0ms preprocess, 61.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 58.6ms\n",
      "Speed: 1.0ms preprocess, 58.6ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 58.1ms\n",
      "Speed: 1.0ms preprocess, 58.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 89.0ms\n",
      "Speed: 0.0ms preprocess, 89.0ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 65.0ms\n",
      "Speed: 1.1ms preprocess, 65.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 59.4ms\n",
      "Speed: 0.0ms preprocess, 59.4ms inference, 0.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 57.0ms\n",
      "Speed: 1.0ms preprocess, 57.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 78.0ms\n",
      "Speed: 1.0ms preprocess, 78.0ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 55.1ms\n",
      "Speed: 0.0ms preprocess, 55.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 90.0ms\n",
      "Speed: 1.0ms preprocess, 90.0ms inference, 1.1ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 55.0ms\n",
      "Speed: 1.1ms preprocess, 55.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 60.5ms\n",
      "Speed: 0.0ms preprocess, 60.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 3 With Helmets, 54.0ms\n",
      "Speed: 1.0ms preprocess, 54.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 68.6ms\n",
      "Speed: 1.0ms preprocess, 68.6ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 3 With Helmets, 63.1ms\n",
      "Speed: 0.0ms preprocess, 63.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 234.6ms\n",
      "Speed: 1.0ms preprocess, 234.6ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 313.2ms\n",
      "Speed: 1.0ms preprocess, 313.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 645.5ms\n",
      "Speed: 3.0ms preprocess, 645.5ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 1 With Helmet, 432.1ms\n",
      "Speed: 1.0ms preprocess, 432.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 128x224 1 Person_Bike, 361.7ms\n",
      "Speed: 1.0ms preprocess, 361.7ms inference, 1.0ms postprocess per image at shape (1, 3, 128, 224)\n",
      "Person_Bike False\n",
      "\n",
      "0: 224x224 2 With Helmets, 391.2ms\n",
      "Speed: 1.0ms preprocess, 391.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Detect person on a bike\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m person_bike_results \u001b[38;5;241m=\u001b[39m \u001b[43mperson_bike_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Process each detection result\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m person_bike_results:\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\engine\\model.py:257\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\engine\\predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\engine\\predictor.py:266\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 266\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\engine\\predictor.py:137\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[0;32m    136\u001b[0m                            mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:356\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    353\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\nn\\tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\nn\\tasks.py:60\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\nn\\tasks.py:81\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 81\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m     82\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\nn\\modules\\head.py:45\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[1;32m---> 45\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:40\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    # Process frame\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Detect person on a bike\n",
    "    person_bike_results = person_bike_model.predict(img)\n",
    "\n",
    "    # Process each detection result\n",
    "    for r in person_bike_results:\n",
    "        boxes = r.boxes\n",
    "        # Filter detections for person on a bike\n",
    "        for box in boxes:\n",
    "            cls = box.cls\n",
    "            print(person_bike_model.names[int(cls)], person_bike_model.names[int(cls)] == \"Person_Bikes\")\n",
    "            if person_bike_model.names[int(cls)] == \"Person_Bike\":\n",
    "                # Crop person on a bike image\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                person_bike_image = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "                # Detect helmet on the person\n",
    "                helmet_results = helmet_model.predict(person_bike_image)\n",
    "\n",
    "                # Process each helmet detection result\n",
    "                for hr in helmet_results:\n",
    "                    h_boxes = hr.boxes\n",
    "                    # Filter detections for no helmet\n",
    "                    for h_bo in h_boxes:\n",
    "                        h_cls = h_bo.cls\n",
    "                        if not helmet_model.names[int(h_cls)] == \"With Helmet\" :\n",
    "                            # Extract number plate from the person bike image\n",
    "                            number_plate_results = number_plate_model.predict(person_bike_image)\n",
    "\n",
    "                            # Process each number plate detection result\n",
    "                            for npr in number_plate_results:\n",
    "                                np_boxes = npr.boxes\n",
    "                                # Filter detections for number plate\n",
    "                                for np_box in np_boxes:\n",
    "                                    np_cls = np_box.cls\n",
    "                                    print(number_plate_model.names[int(np_cls)])\n",
    "                                    if number_plate_model.names[int(np_cls)] == \"number_plate\":\n",
    "                                        # Crop number plate image\n",
    "                                        np_x1, np_y1, np_x2, np_y2 = np_box.xyxy[0]\n",
    "                                        number_plate_image = person_bike_image[int(np_y1):int(np_y2),\n",
    "                                                             int(np_x1):int(np_x2)]\n",
    "                                        # Save the cropped number plate image\n",
    "                                        output_file = f\"person_violation_{image_file}\"\n",
    "                                        output_path = os.path.join(output_dir, output_file)\n",
    "                                        cv2.imwrite(output_path, person_bike_image)\n",
    "\n",
    "                                        # Perform OCR on the number plate image\n",
    "                                        gray = cv2.cvtColor(number_plate_image, cv2.COLOR_BGR2GRAY)\n",
    "                                        text = pytesseract.image_to_string(gray)\n",
    "                                        # Example usage\n",
    "                                        # Create the \"vehicles\" table if it doesn't exist\n",
    "                                        # create_table()\n",
    "                                        # Insert two records into the \"vehicles\" table\n",
    "                                        # insert_record(text, output_path)\n",
    "                                        # Print the extracted text\n",
    "                                        print(\"Number Plate Text:\", text)\n",
    "    cv2.imshow('Face Mask Detection', frame)\n",
    "                                        \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b3d2d-2d97-4a16-80b8-3ec3ad3e1f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
